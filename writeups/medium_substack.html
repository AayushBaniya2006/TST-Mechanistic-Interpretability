<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<style>
  body {
    font-family: Georgia, 'Times New Roman', serif;
    max-width: 740px;
    margin: 40px auto;
    padding: 0 20px;
    line-height: 1.8;
    color: #1a1a1a;
    font-size: 18px;
  }
  h1 { font-size: 2em; margin-bottom: 0.2em; line-height: 1.2; }
  h2 { font-size: 1.4em; margin-top: 2em; }
  h3 { font-size: 1.15em; margin-top: 1.5em; }
  p { margin: 1em 0; }
  img { max-width: 100%; height: auto; margin: 1.5em 0; display: block; }
  figcaption { font-size: 0.85em; color: #666; margin-top: -1em; margin-bottom: 1.5em; font-style: italic; }
  table { border-collapse: collapse; width: 100%; margin: 1.5em 0; font-size: 0.95em; }
  th, td { border: 1px solid #ddd; padding: 8px 12px; text-align: left; }
  th { background: #f5f5f5; font-weight: bold; }
  code { background: #f5f5f5; padding: 2px 6px; border-radius: 3px; font-size: 0.9em; }
  pre { background: #f5f5f5; padding: 16px; border-radius: 4px; overflow-x: auto; font-size: 0.85em; line-height: 1.5; }
  pre code { background: none; padding: 0; }
  blockquote { border-left: 3px solid #ccc; margin-left: 0; padding-left: 1.2em; color: #555; }
  hr { border: none; border-top: 1px solid #ddd; margin: 2.5em 0; }
  a { color: #1a73e8; text-decoration: none; }
  a:hover { text-decoration: underline; }
  .subtitle { font-size: 1.1em; color: #555; margin-top: 0; }
  ul { margin: 1em 0; padding-left: 1.5em; }
  li { margin: 0.3em 0; }
</style>
</head>
<body>

<h1>Most "Important" Attention Heads in Time Series Transformers Are Statistical Noise</h1>

<p class="subtitle"><em>By Aayush Baniya | <a href="https://www.bluedot.org/">BlueDot Impact AI Safety Fundamentals</a></em></p>

<hr>

<p>Activation patching on a Time Series Transformer identified 8 out of 24 attention heads as causally important on a dataset with 96 output classes. After FDR correction, zero survived. Every one was a false positive. The heatmaps looked clean the entire time.</p>

<p>The patching pipeline produced confident, interpretable-looking results that were indistinguishable from noise, and there was nothing in the original framework to flag that.</p>

<hr>

<h2>Background</h2>

<p>I found Matiss Kalnare's <a href="https://github.com/mathiisk/TST-Mechanistic-Interpretability">activation patching framework</a> for Time Series Transformers while looking for a project for the BlueDot Impact AI Safety course. Most mech interp work targets LLMs &mdash; time series models are barely touched, even though they show up in healthcare, finance, and astronomy.</p>

<p>His framework takes a 3-layer encoder with 8 heads per layer (24 total), trained on <a href="http://www.timeseriesclassification.com/">UEA archive</a> classification tasks, and patches individual head outputs between correctly and incorrectly classified inputs to measure the probability shift. You take a correctly classified input and a misclassified one, swap one head's output at a time, and see which swaps change the prediction.</p>

<p>What it lacked: confidence intervals, multiple comparison correction, any stability check. The importance scores were point estimates &mdash; one number per head, no uncertainty quantification, no control for 24 simultaneous tests.</p>

<hr>

<h2>What I Built</h2>

<img src="https://raw.githubusercontent.com/AayushBaniya2006/TST-Mechanistic-Interpretability/main/Results/Summary/figures/pipeline_diagram.png" alt="Pipeline diagram showing the full analysis pipeline">
<figcaption>Full pipeline. The top half is the original framework. The bottom half is my statistical extensions.</figcaption>

<p>About 3,840 lines of code, 174 automated tests. The three main additions:</p>

<p><strong>Bootstrap resampling</strong> &mdash; Instead of trusting one importance score per head, I resampled the data 10,000 times to get 95% confidence intervals. This tells you how much the score would change if you collected slightly different data.</p>

<p><strong>FDR correction</strong> &mdash; When you test 24 heads at p &lt; 0.05, you'd expect about 1.2 false positives by chance. Benjamini-Hochberg adjusts the significance threshold to control for that.</p>

<p><strong>Stability testing</strong> &mdash; I added three types of noise to the inputs (Gaussian noise like sensor error, time warping, phase shifting), re-ran all the patching on the noisy inputs, and checked whether the same heads came out on top. If the rankings change when you barely touch the input, those rankings weren't real.</p>

<hr>

<h2>Results</h2>

<h3>Head Importance</h3>

<img src="https://raw.githubusercontent.com/AayushBaniya2006/TST-Mechanistic-Interpretability/main/Results/Summary/figures/fig1_head_importance.png" alt="Head importance heatmaps across three datasets">
<figcaption>Head importance heatmaps. Left to right: JapaneseVowels, PenDigits, LSST. Stars mark heads surviving FDR correction.</figcaption>

<p><strong>JapaneseVowels</strong> (9 classes) &mdash; 21/24 heads significant after FDR. Cohen&rsquo;s d = 3.35 (large). Stability &rho; = 0.884. Layer 0 heads carried roughly 4x more causal weight than Layer 2. Signal large enough that statistical corrections barely changed the picture.</p>

<p><strong>PenDigits</strong> (10 classes) &mdash; 24/24 significant. d = 1.22 (large). Stability &rho; = 0.894. Every head matters and the rankings hold under perturbation.</p>

<p><strong>LSST</strong> (96-way output: 14 astronomical object types, label indices spanning 0&ndash;95) &mdash; 0/24 significant. d = 0.50 (small). Stability &rho; = 0.484. Head rankings reshuffled under minimal Gaussian noise. The naive analysis (bootstrap CIs without multiple comparison correction) flagged 8 heads. After FDR correction, all were false positives.</p>

<h3>Effect Sizes</h3>

<img src="https://raw.githubusercontent.com/AayushBaniya2006/TST-Mechanistic-Interpretability/main/Results/Summary/figures/fig4_effect_sizes.png" alt="Per-head Cohen's d across all three datasets">
<figcaption>Per-head Cohen's d across all three datasets. JapaneseVowels and PenDigits sit in the medium-to-large range. LSST clusters around zero.</figcaption>

<p>JapaneseVowels and PenDigits heads sit in the medium-to-large range. LSST heads cluster around zero &mdash; most confidence intervals cross the zero line. You can't even tell which direction the effect goes.</p>

<h3>Stability</h3>

<img src="https://raw.githubusercontent.com/AayushBaniya2006/TST-Mechanistic-Interpretability/main/Results/Summary/figures/fig3_stability.png" alt="Mean stability under input perturbations">
<figcaption>Mean stability (Spearman &rho;) under input perturbations. Green dashed line is the 0.7 threshold.</figcaption>

<p>The green dashed line is the 0.7 stability threshold. JapaneseVowels and PenDigits stay well above it &mdash; the same heads matter regardless of how you perturb the input. LSST is at 0.48, below coin-flip territory. Different "important" heads every time.</p>

<p>I tested with three perturbation types (Gaussian noise, time warp, phase shift) at multiple strengths. Bar heights are averages across all perturbations.</p>

<h3>Attention Weights vs Causal Importance</h3>

<img src="https://raw.githubusercontent.com/AayushBaniya2006/TST-Mechanistic-Interpretability/main/Results/Summary/figures/fig2_baseline_comparison.png" alt="Correlation between attribution methods and patching effects">
<figcaption>Correlation between attribution methods and actual patching effects.</figcaption>

<p>Attention entropy and variance correlate poorly with actual patching effects (&rho; &asymp; 0.2 across all three datasets). Maximum attention weight does better on simpler tasks (&rho; = 0.48 on JapaneseVowels) but not on LSST. Integrated Gradients captures moderate signal on JapaneseVowels (&rho; = 0.575) but goes negative on LSST (&rho; = &minus;0.12). Simpler attribution methods don't reliably track causal importance.</p>

<h3>Observed Effects vs the Null</h3>

<img src="https://raw.githubusercontent.com/AayushBaniya2006/TST-Mechanistic-Interpretability/main/Results/Summary/figures/fig5_null_distribution.png" alt="Maximum patching effects per sample pair vs null distribution">
<figcaption>Each dot is one sample pair's maximum patching effect. Black bar is the 95th percentile of the null distribution.</figcaption>

<p>Each dot is one sample pair's maximum patching effect. The black bar is the 95th percentile of the null distribution. JapaneseVowels sits well above. LSST clusters right around the null &mdash; indistinguishable from chance.</p>

<hr>

<h2>Why LSST Fails</h2>

<p>LSST has 14 distinct astronomical object types, but because the label indices are sparse (ranging from 0 to 95), the model uses a 96-way softmax. Probability mass is distributed across 96 output slots, meaning each class gets roughly 1% of the mass. Single-head patching produces shifts that are below the noise floor at that scale.</p>

<p>The method lacks statistical power once the output space dilutes per-head effects below detectability. Individual head contributions are too small relative to the output entropy for single-head patching to resolve them.</p>

<p>This is a complexity-dependent ceiling on head-level causal attribution.</p>

<hr>

<h2>What This Means for Interpretability</h2>

<p>If single-head activation patching breaks down with large output spaces, that constrains how far head-level causal claims generalize. Language models with vocabulary sizes in the tens of thousands face a similar dilution problem in principle &mdash; one hypothesis for why patching works in that setting is that researchers typically measure logit differences on specific tokens rather than full-distribution probability shifts. The output space is effectively narrowed by the metric choice, and that carries assumptions worth stating explicitly.</p>

<p>The LSST heatmaps looked just as structured as the JapaneseVowels heatmaps. Same color gradients, same visual patterns. A reviewer would not have caught the problem without running the corrections. If mechanistic interpretability is going to underwrite safety claims, the evidence standards need to include uncertainty quantification and robustness checks as defaults. Point estimates and heatmaps are hypotheses, not evidence.</p>

<p>Four takeaways:</p>

<p><strong>1. Always correct for multiple comparisons.</strong> Testing N components means expecting N &times; &alpha; false positives. This is basic statistics but is routinely ignored in interpretability papers.</p>

<p><strong>2. Report effect sizes, not just p-values.</strong> LSST's per-head effects were "negligible" &mdash; knowing this is more useful than knowing they failed an FDR threshold.</p>

<p><strong>3. Test stability.</strong> A finding that disappears under small perturbations was never robust. Stability testing should be standard.</p>

<p><strong>4. Don't assume attention = importance.</strong> Attention entropy and variance show &rho; &asymp; 0.2 against causal importance. Even maximum attention weight only reaches moderate correlation on simpler tasks. Interpretability methods based solely on attention patterns will miss much of the causal structure.</p>

<hr>

<h2>Limitations</h2>

<ul>
<li><strong>3 datasets</strong> &mdash; Cannot pin down the exact complexity ceiling. More datasets between 10 and 96 output classes would help.</li>
<li><strong>Single training seed</strong> &mdash; Results may be specific to this model's weight configuration.</li>
<li><strong>Head-level granularity only</strong> &mdash; Position-level patching (which heads matter at which time steps) was explored but not statistically validated at the same depth.</li>
</ul>

<hr>

<h2>Open Questions</h2>

<ul>
<li>Where exactly between 10 and 96 output classes the complexity ceiling sits</li>
<li>Whether group-level patching (multiple heads at once) recovers signal on complex tasks</li>
<li>Whether head importance rankings change across training seeds</li>
<li>How this extends to other Transformer architectures</li>
</ul>

<hr>

<h2>Reproduce It</h2>

<p>Everything is open-source:</p>

<pre><code>git clone https://github.com/AayushBaniya2006/TST-Mechanistic-Interpretability.git
cd TST-Mechanistic-Interpretability
pip install -e ".[dev]"

python Scripts/run_complete_analysis.py     # Full statistical pipeline
python Scripts/run_stability_experiments.py # Stability testing
pytest tests/ -v                            # 174 tests</code></pre>

<p>Seed 42, 10,000 bootstrap resamples, FDR &alpha; = 0.05. Fully reproducible.</p>

<hr>

<h2>Technical Details</h2>

<ul>
<li><strong>Model</strong>: 3-layer Transformer encoder, 8 heads/layer, 24 total</li>
<li><strong>Patching</strong>: Head-level output swap between correct/incorrect pairs</li>
<li><strong>Statistics</strong>: Bootstrap CIs (10k), Benjamini-Hochberg FDR (&alpha; = 0.05), Cohen's d</li>
<li><strong>Stability</strong>: Gaussian noise (&sigma; = 0.05&ndash;0.20), time warp (0.05&ndash;0.20), phase shift (1&ndash;3 steps)</li>
<li><strong>Baselines</strong>: Integrated Gradients (via Captum), attention weights (entropy/max/variance)</li>
<li><strong>Code</strong>: ~3,840 lines of statistical extensions, 174 automated tests</li>
</ul>

<hr>

<p><em>This project was completed through <a href="https://www.bluedot.org/">BlueDot Impact AI Safety Fundamentals</a>. The original activation patching framework was built by <a href="https://github.com/mathiisk/TST-Mechanistic-Interpretability">Matiss Kalnare</a>. I built the statistical validation, stability testing, baseline comparisons, and automation pipeline.</em></p>

<p><em><a href="https://github.com/AayushBaniya2006/TST-Mechanistic-Interpretability">GitHub repo</a></em></p>

</body>
</html>
