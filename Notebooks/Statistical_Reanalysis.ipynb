{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Reanalysis of Patching Results\n",
    "\n",
    "This notebook applies rigorous statistical analysis to all existing patching results, addressing the methodological gaps identified in the assessment:\n",
    "\n",
    "1. **Confidence Intervals**: BCa bootstrap CIs for all claims\n",
    "2. **Effect Sizes**: Cohen's d with interpretation\n",
    "3. **FDR Correction**: Benjamini-Hochberg for multiple comparisons\n",
    "4. **Random Baseline**: Null distribution for statistical significance\n",
    "5. **Proper Framing**: Case studies, not generalizable correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Our statistical infrastructure\n",
    "from Utilities.statistics import (\n",
    "    compute_confidence_interval,\n",
    "    compute_effect_size,\n",
    "    apply_fdr_correction,\n",
    "    compute_correlation_ci,\n",
    "    significance_test,\n",
    "    power_analysis,\n",
    "    compute_aggregate_statistics,\n",
    "    format_result_table\n",
    ")\n",
    "\n",
    "from Utilities.baselines import (\n",
    "    random_patching_baseline,\n",
    "    compute_empirical_p_value\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load aggregate stability results\n",
    "stability_df = pd.read_csv('../Results/Stability/Summary/aggregate_results.csv')\n",
    "print(\"Stability Results:\")\n",
    "display(stability_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load all raw results\n",
    "def load_all_raw_results(base_path: Path) -> dict:\n",
    "    \"\"\"Load all raw NPZ files organized by dataset.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for dataset in ['JapaneseVowels', 'PenDigits', 'LSST']:\n",
    "        results[dataset] = {'denoise': [], 'noise': []}\n",
    "        dataset_path = base_path / dataset\n",
    "        \n",
    "        for mode in ['denoise', 'noise']:\n",
    "            mode_path = dataset_path / mode\n",
    "            if not mode_path.exists():\n",
    "                continue\n",
    "                \n",
    "            for class_dir in mode_path.iterdir():\n",
    "                if not class_dir.is_dir() or not class_dir.name.startswith('class_'):\n",
    "                    continue\n",
    "                    \n",
    "                class_id = int(class_dir.name.split('_')[1])\n",
    "                \n",
    "                for pair_dir in class_dir.iterdir():\n",
    "                    if not pair_dir.is_dir() or not pair_dir.name.startswith('pair_'):\n",
    "                        continue\n",
    "                    \n",
    "                    npz_path = pair_dir / 'raw_results.npz'\n",
    "                    if npz_path.exists():\n",
    "                        data = np.load(npz_path)\n",
    "                        results[dataset][mode].append({\n",
    "                            'class_id': class_id,\n",
    "                            'pair_name': pair_dir.name,\n",
    "                            'baseline': data['baseline'],\n",
    "                            'head_patch': data['head_patch'] if 'head_patch' in data else None,\n",
    "                            'layer_patch': data['layer_patch'] if 'layer_patch' in data else None,\n",
    "                        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "base_path = Path('../Results')\n",
    "all_results = load_all_raw_results(base_path)\n",
    "\n",
    "# Summary counts\n",
    "for dataset, modes in all_results.items():\n",
    "    denoise_count = len(modes['denoise'])\n",
    "    noise_count = len(modes['noise'])\n",
    "    print(f\"{dataset}: {denoise_count} denoise pairs, {noise_count} noise pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Delta P with Confidence Intervals\n",
    "\n",
    "For each dataset, compute mean ΔP across all pairs with BCa bootstrap 95% CIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_delta_p_with_ci(results: dict, mode: str = 'denoise') -> pd.DataFrame:\n",
    "    \"\"\"Compute mean ΔP with 95% CI for each dataset.\"\"\"\n",
    "    summary = []\n",
    "    \n",
    "    for dataset, modes in results.items():\n",
    "        pairs = modes[mode]\n",
    "        if not pairs:\n",
    "            continue\n",
    "        \n",
    "        # Collect all delta_p values across all heads and pairs\n",
    "        all_delta_p = []\n",
    "        max_delta_p_per_pair = []\n",
    "        \n",
    "        for pair in pairs:\n",
    "            if pair['head_patch'] is None:\n",
    "                continue\n",
    "            \n",
    "            true_label = pair['class_id']\n",
    "            baseline_p = pair['baseline'][true_label]\n",
    "            \n",
    "            # Delta P for each head\n",
    "            head_delta = pair['head_patch'][:, :, true_label] - baseline_p\n",
    "            all_delta_p.extend(head_delta.flatten().tolist())\n",
    "            max_delta_p_per_pair.append(head_delta.max())\n",
    "        \n",
    "        if not all_delta_p:\n",
    "            continue\n",
    "        \n",
    "        all_delta_p = np.array(all_delta_p)\n",
    "        max_delta_p_per_pair = np.array(max_delta_p_per_pair)\n",
    "        \n",
    "        # Compute CI for mean delta_p\n",
    "        mean_ci = compute_confidence_interval(all_delta_p, confidence=0.95, method='bca')\n",
    "        max_ci = compute_confidence_interval(max_delta_p_per_pair, confidence=0.95, method='bca')\n",
    "        \n",
    "        summary.append({\n",
    "            'Dataset': dataset,\n",
    "            'n_pairs': len(pairs),\n",
    "            'n_observations': len(all_delta_p),\n",
    "            'mean_delta_p': mean_ci.mean,\n",
    "            'mean_ci_lower': mean_ci.lower,\n",
    "            'mean_ci_upper': mean_ci.upper,\n",
    "            'max_delta_p_mean': max_ci.mean,\n",
    "            'max_ci_lower': max_ci.lower,\n",
    "            'max_ci_upper': max_ci.upper,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "delta_p_summary = compute_delta_p_with_ci(all_results, 'denoise')\n",
    "print(\"\\n=== Delta P Results with 95% CIs (Denoise Mode) ===\")\n",
    "display(delta_p_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization with error bars\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "datasets = delta_p_summary['Dataset'].values\n",
    "means = delta_p_summary['mean_delta_p'].values\n",
    "lower_err = means - delta_p_summary['mean_ci_lower'].values\n",
    "upper_err = delta_p_summary['mean_ci_upper'].values - means\n",
    "\n",
    "x = np.arange(len(datasets))\n",
    "ax.bar(x, means, yerr=[lower_err, upper_err], capsize=5, color=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(datasets)\n",
    "ax.set_ylabel('Mean ΔP')\n",
    "ax.set_title('Mean Patching Effect (ΔP) with 95% Bootstrap CIs')\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Add annotation\n",
    "for i, (d, m, l, u) in enumerate(zip(datasets, means, delta_p_summary['mean_ci_lower'], delta_p_summary['mean_ci_upper'])):\n",
    "    ax.annotate(f'{m:.3f}\\n[{l:.3f}, {u:.3f}]', \n",
    "                xy=(i, m + upper_err[i] + 0.02), \n",
    "                ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Results/Summary/delta_p_with_ci.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Head Importance with FDR Correction\n",
    "\n",
    "Apply Benjamini-Hochberg correction to head importance rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_head_importance_with_fdr(results: dict, mode: str = 'denoise') -> dict:\n",
    "    \"\"\"Compute head importance with FDR correction.\"\"\"\n",
    "    fdr_results = {}\n",
    "    \n",
    "    for dataset, modes in results.items():\n",
    "        pairs = modes[mode]\n",
    "        if not pairs:\n",
    "            continue\n",
    "        \n",
    "        # Aggregate delta_p per head across all pairs\n",
    "        head_deltas = {}  # (layer, head) -> list of delta_p values\n",
    "        \n",
    "        for pair in pairs:\n",
    "            if pair['head_patch'] is None:\n",
    "                continue\n",
    "            \n",
    "            true_label = pair['class_id']\n",
    "            baseline_p = pair['baseline'][true_label]\n",
    "            head_delta = pair['head_patch'][:, :, true_label] - baseline_p\n",
    "            \n",
    "            n_layers, n_heads = head_delta.shape\n",
    "            for l in range(n_layers):\n",
    "                for h in range(n_heads):\n",
    "                    key = (l, h)\n",
    "                    if key not in head_deltas:\n",
    "                        head_deltas[key] = []\n",
    "                    head_deltas[key].append(head_delta[l, h])\n",
    "        \n",
    "        # For each head, test if mean delta_p > 0\n",
    "        p_values = []\n",
    "        head_stats = []\n",
    "        \n",
    "        for (l, h), deltas in head_deltas.items():\n",
    "            deltas = np.array(deltas)\n",
    "            mean_delta = np.mean(deltas)\n",
    "            \n",
    "            # One-sample t-test against 0\n",
    "            if len(deltas) > 1:\n",
    "                t_stat, p_val = stats.ttest_1samp(deltas, 0, alternative='greater')\n",
    "            else:\n",
    "                p_val = 1.0\n",
    "                t_stat = 0\n",
    "            \n",
    "            p_values.append(p_val)\n",
    "            ci = compute_confidence_interval(deltas, confidence=0.95)\n",
    "            \n",
    "            head_stats.append({\n",
    "                'layer': l,\n",
    "                'head': h,\n",
    "                'mean_delta_p': mean_delta,\n",
    "                'ci_lower': ci.lower,\n",
    "                'ci_upper': ci.upper,\n",
    "                'n_pairs': len(deltas),\n",
    "                'p_value': p_val\n",
    "            })\n",
    "        \n",
    "        # Apply FDR correction\n",
    "        fdr_result = apply_fdr_correction(p_values, method='benjamini_hochberg')\n",
    "        \n",
    "        for i, stat in enumerate(head_stats):\n",
    "            stat['p_value_corrected'] = fdr_result['p_corrected'][i]\n",
    "            stat['significant_fdr'] = fdr_result['significant'][i]\n",
    "        \n",
    "        fdr_results[dataset] = {\n",
    "            'head_stats': pd.DataFrame(head_stats).sort_values('mean_delta_p', ascending=False),\n",
    "            'n_significant': fdr_result['n_significant'],\n",
    "            'n_tests': fdr_result['n_tests']\n",
    "        }\n",
    "    \n",
    "    return fdr_results\n",
    "\n",
    "fdr_results = compute_head_importance_with_fdr(all_results, 'denoise')\n",
    "\n",
    "for dataset, result in fdr_results.items():\n",
    "    print(f\"\\n=== {dataset}: Head Importance with FDR Correction ===\")\n",
    "    print(f\"Significant heads after FDR: {result['n_significant']}/{result['n_tests']}\")\n",
    "    display(result['head_stats'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stability Analysis with Proper Statistics\n",
    "\n",
    "Reanalyze stability results with:\n",
    "- CIs for all metrics\n",
    "- Honest framing as case studies (n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-dataset stability summaries with CIs\n",
    "def compute_stability_with_ci(stability_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute stability metrics with 95% CIs per dataset.\"\"\"\n",
    "    summaries = []\n",
    "    \n",
    "    for dataset in stability_df['dataset'].unique():\n",
    "        subset = stability_df[stability_df['dataset'] == dataset]\n",
    "        \n",
    "        rank_corrs = subset['rank_corr'].values\n",
    "        top5_overlaps = subset['top5'].values\n",
    "        stabilities = subset['stability'].values\n",
    "        \n",
    "        # CIs (may fail with very small n)\n",
    "        n = len(rank_corrs)\n",
    "        \n",
    "        if n >= 2:\n",
    "            rank_ci = compute_confidence_interval(rank_corrs, confidence=0.95)\n",
    "            top5_ci = compute_confidence_interval(top5_overlaps, confidence=0.95)\n",
    "            stab_ci = compute_confidence_interval(stabilities, confidence=0.95)\n",
    "        else:\n",
    "            # Can't compute CI with n < 2\n",
    "            rank_ci = type('obj', (object,), {'mean': rank_corrs[0] if n > 0 else np.nan, 'lower': np.nan, 'upper': np.nan})()\n",
    "            top5_ci = type('obj', (object,), {'mean': top5_overlaps[0] if n > 0 else np.nan, 'lower': np.nan, 'upper': np.nan})()\n",
    "            stab_ci = type('obj', (object,), {'mean': stabilities[0] if n > 0 else np.nan, 'lower': np.nan, 'upper': np.nan})()\n",
    "        \n",
    "        summaries.append({\n",
    "            'Dataset': dataset,\n",
    "            'n_perturbations': n,\n",
    "            'rank_corr_mean': rank_ci.mean,\n",
    "            'rank_corr_ci': f\"[{rank_ci.lower:.3f}, {rank_ci.upper:.3f}]\" if not np.isnan(rank_ci.lower) else \"N/A\",\n",
    "            'top5_mean': top5_ci.mean,\n",
    "            'top5_ci': f\"[{top5_ci.lower:.3f}, {top5_ci.upper:.3f}]\" if not np.isnan(top5_ci.lower) else \"N/A\",\n",
    "            'stability_mean': stab_ci.mean,\n",
    "            'stability_ci': f\"[{stab_ci.lower:.3f}, {stab_ci.upper:.3f}]\" if not np.isnan(stab_ci.lower) else \"N/A\",\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summaries)\n",
    "\n",
    "stability_summary = compute_stability_with_ci(stability_df)\n",
    "print(\"\\n=== Stability Summary with 95% CIs ===\")\n",
    "display(stability_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The n=3 Problem: Honest Assessment\n",
    "\n",
    "With only 3 datasets, any correlation claim is statistically meaningless. This section documents the limitation and reframes findings as case studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the n=3 problem\n",
    "print(\"=== THE n=3 PROBLEM ===\")\n",
    "print()\n",
    "\n",
    "# Our observed values\n",
    "delta_p_values = [0.59, 0.15, 0.01]  # JapaneseVowels, PenDigits, LSST\n",
    "stability_values = [0.89, 0.87, 0.52]  # Mean stability\n",
    "\n",
    "observed_rho, observed_p = stats.spearmanr(delta_p_values, stability_values)\n",
    "print(f\"Observed Spearman ρ = {observed_rho:.3f}, p = {observed_p:.4f}\")\n",
    "print()\n",
    "\n",
    "# Simulate: how often do we get ρ > 0.8 with random data?\n",
    "n_simulations = 10000\n",
    "high_corr_count = 0\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    random_x = np.random.uniform(0, 1, 3)\n",
    "    random_y = np.random.uniform(0, 1, 3)\n",
    "    rho, _ = stats.spearmanr(random_x, random_y)\n",
    "    if abs(rho) > 0.8:\n",
    "        high_corr_count += 1\n",
    "\n",
    "false_positive_rate = high_corr_count / n_simulations\n",
    "print(f\"False positive rate (|ρ| > 0.8 by chance with n=3): {false_positive_rate:.1%}\")\n",
    "print()\n",
    "print(\"CONCLUSION: With n=3, correlation claims are statistically meaningless.\")\n",
    "print(\"A correlation of ρ = 0.89 could easily arise by chance (~20% of the time).\")\n",
    "print()\n",
    "print(\"REFRAMING: We present three case studies that suggest a possible pattern:\")\n",
    "print(\"  - Datasets with stronger patching effects show higher stability\")\n",
    "print(\"  - This preliminary observation warrants investigation with more datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power analysis: how many datasets would we need?\n",
    "print(\"\\n=== POWER ANALYSIS ===\")\n",
    "print()\n",
    "\n",
    "# For different effect sizes (correlations), how many data points needed?\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def required_n_for_correlation(r: float, alpha: float = 0.05, power: float = 0.80) -> int:\n",
    "    \"\"\"Calculate required n for detecting correlation r.\"\"\"\n",
    "    from scipy.stats import norm\n",
    "    \n",
    "    # Fisher's z transformation\n",
    "    z = 0.5 * np.log((1 + r) / (1 - r))\n",
    "    z_alpha = norm.ppf(1 - alpha/2)\n",
    "    z_beta = norm.ppf(power)\n",
    "    \n",
    "    n = ((z_alpha + z_beta) / z) ** 2 + 3\n",
    "    return int(np.ceil(n))\n",
    "\n",
    "print(\"Required n for 80% power to detect correlation:\")\n",
    "for r in [0.9, 0.8, 0.7, 0.6, 0.5]:\n",
    "    n_required = required_n_for_correlation(r)\n",
    "    print(f\"  ρ = {r}: n = {n_required} datasets\")\n",
    "\n",
    "print()\n",
    "print(f\"Current n = 3. For ρ ≈ 0.89, we need n ≈ {required_n_for_correlation(0.89)} datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Publication-Ready Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1: Main Results with 95% CIs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TABLE 1: Main Results with 95% Bootstrap CIs\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "table1 = []\n",
    "for i, row in delta_p_summary.iterrows():\n",
    "    dataset = row['Dataset']\n",
    "    stab_row = stability_summary[stability_summary['Dataset'] == dataset].iloc[0]\n",
    "    \n",
    "    table1.append({\n",
    "        'Dataset': dataset,\n",
    "        'n pairs': row['n_pairs'],\n",
    "        'Mean ΔP': f\"{row['mean_delta_p']:.3f} [{row['mean_ci_lower']:.3f}, {row['mean_ci_upper']:.3f}]\",\n",
    "        'Stability (ρ)': f\"{stab_row['rank_corr_mean']:.3f} {stab_row['rank_corr_ci']}\",\n",
    "        'Top-5 Overlap': f\"{stab_row['top5_mean']:.3f} {stab_row['top5_ci']}\"\n",
    "    })\n",
    "\n",
    "table1_df = pd.DataFrame(table1)\n",
    "print(table1_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2: Head Importance with FDR (Top 10 per dataset)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TABLE 2: Top-10 Most Influential Heads (FDR-corrected p-values)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for dataset, result in fdr_results.items():\n",
    "    print(f\"\\n{dataset}:\")\n",
    "    top10 = result['head_stats'].head(10)[['layer', 'head', 'mean_delta_p', 'ci_lower', 'ci_upper', 'p_value_corrected', 'significant_fdr']]\n",
    "    top10.columns = ['Layer', 'Head', 'Mean ΔP', 'CI Lower', 'CI Upper', 'p (FDR)', 'Sig.']\n",
    "    print(top10.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Summary directory if it doesn't exist\n",
    "summary_dir = Path('../Results/Summary')\n",
    "summary_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save all statistical results\n",
    "delta_p_summary.to_csv(summary_dir / 'delta_p_with_ci.csv', index=False)\n",
    "stability_summary.to_csv(summary_dir / 'stability_with_ci.csv', index=False)\n",
    "table1_df.to_csv(summary_dir / 'main_results_table.csv', index=False)\n",
    "\n",
    "# Save FDR results per dataset\n",
    "for dataset, result in fdr_results.items():\n",
    "    result['head_stats'].to_csv(summary_dir / f'{dataset}_head_importance_fdr.csv', index=False)\n",
    "\n",
    "print(\"Results saved to Results/Summary/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions & Limitations\n",
    "\n",
    "### Key Findings (with proper statistical reporting)\n",
    "\n",
    "1. **JapaneseVowels** shows strong patching effects (mean ΔP with 95% CI above zero) and high stability\n",
    "2. **PenDigits** shows moderate effects with moderate stability  \n",
    "3. **LSST** shows near-zero effects with low stability\n",
    "\n",
    "### Critical Limitations\n",
    "\n",
    "1. **Sample Size (n=3 datasets)**: Cannot make correlation claims across datasets\n",
    "2. **Single Training Run**: Cannot distinguish task-specific vs run-specific mechanisms\n",
    "3. **No Null Baseline Yet**: Need random patching comparison for significance\n",
    "\n",
    "### Recommended Framing\n",
    "\n",
    "> \"Through three case studies, we observe that datasets with stronger patching effects \n",
    "> (JapaneseVowels: ΔP = 0.XX [CI], PenDigits: ΔP = 0.XX [CI]) also exhibit higher mechanism\n",
    "> stability under perturbation, while LSST shows neither strong effects nor stability.\n",
    "> This preliminary pattern warrants further investigation with additional datasets.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
