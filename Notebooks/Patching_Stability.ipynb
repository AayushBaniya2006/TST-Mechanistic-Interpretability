{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanistic Stability Analysis\n",
    "\n",
    "This notebook tests whether the model's internal \"explanations\" (which components are important) remain stable when inputs are slightly perturbed but predictions stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from Utilities.TST_trainer import TimeSeriesTransformer, load_dataset\n",
    "from Utilities.utils import sweep_heads, get_probs\n",
    "from Utilities.perturbations import (\n",
    "    apply_perturbation, validate_perturbation, get_perturbation_configs\n",
    ")\n",
    "from Utilities.stability_metrics import (\n",
    "    get_head_importance, compute_all_metrics, \n",
    "    plot_importance_comparison, create_summary_table\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset to analyze\n",
    "DATASET_NAME = 'JapaneseVowels'\n",
    "MODEL_PATH = f'../TST_models/TST_{DATASET_NAME.lower()}.pth'\n",
    "\n",
    "# Output directory\n",
    "RESULTS_DIR = Path(f'../Results/Stability/{DATASET_NAME}')\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f'Dataset: {DATASET_NAME}')\n",
    "print(f'Results will be saved to: {RESULTS_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_loader, test_loader = load_dataset(DATASET_NAME, batch_size=32)\n",
    "\n",
    "# Get full test set\n",
    "X_test = test_loader.dataset.tensors[0]\n",
    "y_test = test_loader.dataset.tensors[1]\n",
    "\n",
    "# Determine dimensions\n",
    "train_labels = train_loader.dataset.tensors[1]\n",
    "test_labels = test_loader.dataset.tensors[1]\n",
    "num_classes = int(torch.cat([train_labels, test_labels]).max().item()) + 1\n",
    "seq_len, channels = X_test.shape[1], X_test.shape[2]\n",
    "\n",
    "print(f'Test set: {X_test.shape[0]} samples')\n",
    "print(f'Sequence length: {seq_len}, Channels: {channels}')\n",
    "print(f'Number of classes: {num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = TimeSeriesTransformer(\n",
    "    input_dim=channels,\n",
    "    num_classes=num_classes,\n",
    "    seq_len=seq_len\n",
    ")\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu', weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# Model info\n",
    "num_layers = len(model.transformer_encoder.layers)\n",
    "num_heads = model.transformer_encoder.layers[0].self_attn.num_heads\n",
    "\n",
    "print(f'Model loaded: {num_layers} layers, {num_heads} heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validate Perturbations\n",
    "\n",
    "Check which perturbation settings preserve accuracy (within 5% drop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = get_perturbation_configs()\n",
    "valid_configs = {}\n",
    "\n",
    "print('Validating perturbations (must preserve accuracy within 5%):\\n')\n",
    "\n",
    "for method, param_list in configs.items():\n",
    "    print(f'{method.upper()}:')\n",
    "    valid_configs[method] = []\n",
    "    \n",
    "    for params in param_list:\n",
    "        X_pert = apply_perturbation(X_test, method, seed=SEED, **params)\n",
    "        result = validate_perturbation(model, X_test, X_pert, y_test)\n",
    "        \n",
    "        param_str = ', '.join(f'{k}={v}' for k, v in params.items())\n",
    "        status = 'PASS' if result['valid'] else 'FAIL'\n",
    "        \n",
    "        print(f\"  {param_str:20s} | Acc: {result['original_acc']:.3f} -> {result['perturbed_acc']:.3f} | {status}\")\n",
    "        \n",
    "        if result['valid']:\n",
    "            valid_configs[method].append(params)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Select Sample Pairs for Analysis\n",
    "\n",
    "Pick pairs of (clean, corrupt) samples where the model makes different predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model predictions\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test).argmax(dim=1)\n",
    "\n",
    "# Find correctly and incorrectly classified samples\n",
    "correct_mask = (preds == y_test)\n",
    "correct_idx = torch.where(correct_mask)[0].numpy()\n",
    "incorrect_idx = torch.where(~correct_mask)[0].numpy()\n",
    "\n",
    "print(f'Correctly classified: {len(correct_idx)}')\n",
    "print(f'Incorrectly classified: {len(incorrect_idx)}')\n",
    "\n",
    "# Select pairs: use correctly classified as \"clean\", pick another sample as \"corrupt\"\n",
    "NUM_PAIRS = min(20, len(correct_idx))\n",
    "np.random.seed(SEED)\n",
    "clean_indices = np.random.choice(correct_idx, NUM_PAIRS, replace=False)\n",
    "corrupt_indices = np.random.choice(correct_idx, NUM_PAIRS, replace=False)\n",
    "\n",
    "# Make sure they're different\n",
    "for i in range(NUM_PAIRS):\n",
    "    while corrupt_indices[i] == clean_indices[i]:\n",
    "        corrupt_indices[i] = np.random.choice(correct_idx)\n",
    "\n",
    "print(f'\\nSelected {NUM_PAIRS} pairs for analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Stability Analysis\n",
    "\n",
    "For each perturbation config:\n",
    "1. Perturb the \"corrupt\" input\n",
    "2. Run patching analysis on (clean, corrupt) and (clean, perturbed)\n",
    "3. Compare the head importance rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "for method, param_list in valid_configs.items():\n",
    "    for params in param_list:\n",
    "        config_name = f\"{method}_\" + \"_\".join(f\"{v}\" for v in params.values())\n",
    "        print(f'\\nRunning: {config_name}')\n",
    "        \n",
    "        # Store metrics for each pair\n",
    "        pair_metrics = []\n",
    "        \n",
    "        for i in range(NUM_PAIRS):\n",
    "            clean_idx = clean_indices[i]\n",
    "            corrupt_idx = corrupt_indices[i]\n",
    "            \n",
    "            clean = X_test[clean_idx:clean_idx+1]\n",
    "            corrupt = X_test[corrupt_idx:corrupt_idx+1]\n",
    "            true_label = y_test[corrupt_idx].item()\n",
    "            \n",
    "            # Perturb the corrupt input\n",
    "            corrupt_pert = apply_perturbation(corrupt, method, seed=SEED+i, **params)\n",
    "            \n",
    "            # Baseline patching: clean -> corrupt\n",
    "            baseline_probs = sweep_heads(model, clean, corrupt, num_classes)\n",
    "            baseline_raw = get_probs(model, corrupt)\n",
    "            baseline_imp = get_head_importance(baseline_probs, baseline_raw, true_label)\n",
    "            \n",
    "            # Perturbed patching: clean -> perturbed_corrupt  \n",
    "            perturbed_probs = sweep_heads(model, clean, corrupt_pert, num_classes)\n",
    "            perturbed_raw = get_probs(model, corrupt_pert)\n",
    "            perturbed_imp = get_head_importance(perturbed_probs, perturbed_raw, true_label)\n",
    "            \n",
    "            # Compute metrics\n",
    "            metrics = compute_all_metrics(baseline_imp, perturbed_imp)\n",
    "            pair_metrics.append(metrics)\n",
    "        \n",
    "        # Average metrics across pairs\n",
    "        avg_metrics = {}\n",
    "        for key in pair_metrics[0].keys():\n",
    "            if isinstance(pair_metrics[0][key], dict):\n",
    "                avg_metrics[key] = {}\n",
    "                for subkey in pair_metrics[0][key].keys():\n",
    "                    avg_metrics[key][subkey] = np.mean([m[key][subkey] for m in pair_metrics])\n",
    "            else:\n",
    "                avg_metrics[key] = np.mean([m[key] for m in pair_metrics])\n",
    "        \n",
    "        all_results[config_name] = avg_metrics\n",
    "        print(f\"  Rank corr: {avg_metrics['rank_correlation']:.3f}, Top-5: {avg_metrics['topk_overlap_k5']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_table = create_summary_table(all_results)\n",
    "print(summary_table)\n",
    "\n",
    "# Save to file\n",
    "with open(RESULTS_DIR / 'summary_table.md', 'w') as f:\n",
    "    f.write(f'# Stability Results: {DATASET_NAME}\\n\\n')\n",
    "    f.write(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for easier analysis\n",
    "rows = []\n",
    "for config, metrics in all_results.items():\n",
    "    row = {\n",
    "        'config': config,\n",
    "        'rank_correlation': metrics['rank_correlation'],\n",
    "        'topk_overlap_k3': metrics['topk_overlap_k3'],\n",
    "        'topk_overlap_k5': metrics['topk_overlap_k5'],\n",
    "        'topk_overlap_k10': metrics['topk_overlap_k10'],\n",
    "        'stability_score': metrics['stability_score']\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(RESULTS_DIR / 'stability_metrics.csv', index=False)\n",
    "print(f'Saved to {RESULTS_DIR / \"stability_metrics.csv\"}')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of rank correlations\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "configs = list(all_results.keys())\n",
    "correlations = [all_results[c]['rank_correlation'] for c in configs]\n",
    "\n",
    "colors = ['blue' if 'gaussian' in c else 'green' if 'time' in c else 'red' for c in configs]\n",
    "ax.bar(configs, correlations, color=colors)\n",
    "ax.axhline(y=0.8, color='gray', linestyle='--', label='High stability threshold')\n",
    "ax.set_ylabel('Rank Correlation')\n",
    "ax.set_xlabel('Perturbation Config')\n",
    "ax.set_title(f'Mechanism Stability: {DATASET_NAME}')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'rank_correlation_bar.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-K overlap comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(configs))\n",
    "width = 0.25\n",
    "\n",
    "for i, k in enumerate([3, 5, 10]):\n",
    "    values = [all_results[c][f'topk_overlap_k{k}'] for c in configs]\n",
    "    ax.bar(x + i*width, values, width, label=f'K={k}')\n",
    "\n",
    "ax.set_ylabel('Jaccard Overlap')\n",
    "ax.set_xlabel('Perturbation Config')\n",
    "ax.set_title(f'Top-K Head Overlap: {DATASET_NAME}')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(configs, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'topk_overlap_bar.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Case Study: Example Comparison\n",
    "\n",
    "Visualize how head importance changes for a specific example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick first valid perturbation for case study\n",
    "case_method = list(valid_configs.keys())[0]\n",
    "case_params = valid_configs[case_method][0]\n",
    "case_config = f\"{case_method}_\" + \"_\".join(f\"{v}\" for v in case_params.values())\n",
    "\n",
    "# Use first pair\n",
    "clean_idx = clean_indices[0]\n",
    "corrupt_idx = corrupt_indices[0]\n",
    "\n",
    "clean = X_test[clean_idx:clean_idx+1]\n",
    "corrupt = X_test[corrupt_idx:corrupt_idx+1]\n",
    "true_label = y_test[corrupt_idx].item()\n",
    "\n",
    "# Perturb\n",
    "corrupt_pert = apply_perturbation(corrupt, case_method, seed=SEED, **case_params)\n",
    "\n",
    "# Get importance\n",
    "baseline_probs = sweep_heads(model, clean, corrupt, num_classes)\n",
    "baseline_raw = get_probs(model, corrupt)\n",
    "baseline_imp = get_head_importance(baseline_probs, baseline_raw, true_label)\n",
    "\n",
    "perturbed_probs = sweep_heads(model, clean, corrupt_pert, num_classes)\n",
    "perturbed_raw = get_probs(model, corrupt_pert)\n",
    "perturbed_imp = get_head_importance(perturbed_probs, perturbed_raw, true_label)\n",
    "\n",
    "# Plot comparison\n",
    "fig = plot_importance_comparison(\n",
    "    baseline_imp, perturbed_imp,\n",
    "    title=f'Case Study: {case_config}',\n",
    "    save_path=str(RESULTS_DIR / 'case_study_comparison.png')\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall statistics\n",
    "avg_rank_corr = df['rank_correlation'].mean()\n",
    "avg_topk5 = df['topk_overlap_k5'].mean()\n",
    "avg_stability = df['stability_score'].mean()\n",
    "\n",
    "findings = f\"\"\"# Stability Analysis Findings: {DATASET_NAME}\n",
    "\n",
    "## Summary Statistics\n",
    "- Average rank correlation: {avg_rank_corr:.3f}\n",
    "- Average top-5 overlap: {avg_topk5:.3f}\n",
    "- Average stability score: {avg_stability:.3f}\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if avg_rank_corr > 0.7:\n",
    "    findings += \"- Mechanistic explanations appear **relatively stable** under tested perturbations\\n\"\n",
    "elif avg_rank_corr > 0.4:\n",
    "    findings += \"- Mechanistic explanations show **moderate stability** - some heads consistently important\\n\"\n",
    "else:\n",
    "    findings += \"- Mechanistic explanations appear **unstable** - different heads become important under perturbation\\n\"\n",
    "\n",
    "if avg_topk5 > 0.5:\n",
    "    findings += \"- Top-5 most important heads show **good overlap** across conditions\\n\"\n",
    "else:\n",
    "    findings += \"- Top-5 most important heads show **limited overlap** - explanations are context-dependent\\n\"\n",
    "\n",
    "findings += f\"\"\"\\n## Perturbation-Specific Results\n",
    "\n",
    "{summary_table}\n",
    "\n",
    "## Notes\n",
    "- Analysis based on {NUM_PAIRS} sample pairs\n",
    "- Random seed: {SEED}\n",
    "\"\"\"\n",
    "\n",
    "print(findings)\n",
    "\n",
    "with open(RESULTS_DIR / 'findings.md', 'w') as f:\n",
    "    f.write(findings)\n",
    "\n",
    "print(f'\\nSaved to {RESULTS_DIR / \"findings.md\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Done!\n",
    "\n",
    "Results saved to `Results/Stability/{DATASET_NAME}/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
